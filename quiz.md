# Quiz

### Question 1: What is the primary purpose of word embeddings in NLP?

- A. To count the frequency of words in a document
- B. To represent words as high-dimensional one-hot vectors
- C. To represent words as dense vectors that capture semantic meaning
- D. To translate text between different languages

### Question 2: Which of the following is NOT a characteristic of Word2Vec?

- A. It learns word representations based on the context they appear in
- B. It can capture semantic relationships between words
- C. It requires manually labeled training data
- D. It can be implemented using either CBOW or Skip-gram architectures

### Question 3: What is the main advantage of GloVe over traditional count-based methods?

- A. It combines global matrix factorization with local context window methods
- B. It only requires a small training corpus
- C. It runs faster because it doesn't use neural networks
- D. It was specifically designed for sentiment analysis tasks

### Question 4: When would you use BERT instead of Word2Vec?

- A. When you need extremely fast word embeddings
- B. When you need context-sensitive representations of words
- C. When you have a very small dataset
- D. When memory efficiency is your primary concern

### Question 5: What is transfer learning in the context of NLP?

- A. Converting text from one language to another
- B. Using knowledge from one task to improve performance on another task
- C. Transferring data between different computing systems
- D. Converting between different word embedding formats

### Question 6: What is a key innovation of the Transformer architecture?

- A. It uses recurrent connections to process sequential data
- B. It uses attention mechanisms to weigh the importance of different words
- C. It can only process fixed-length inputs
- D. It requires less training data than previous models

### Question 7: How does BERT handle the context of words differently from Word2Vec?

- A. BERT represents each word based on all other words in the sentence
- B. BERT only looks at the previous words in a sentence
- C. BERT uses a fixed context window of 5 words
- D. BERT doesn't consider context at all

### Question 8: What task does the masked language model (MLM) in BERT perform during pre-training?

- A. Translating text to different languages
- B. Predicting the next word in a sentence
- C. Predicting randomly masked words in a sentence
- D. Classifying the sentiment of a sentence

### Question 9: What is fine-tuning in the context of pre-trained language models?

- A. Adjusting hyperparameters to improve training speed
- B. Adapting a pre-trained model to a specific downstream task
- C. Reducing the size of the model for deployment
- D. Correcting errors in the model's predictions

### Question 10: Which of these is NOT a common application of transformer-based models like BERT?

- A. Text classification
- B. Named entity recognition
- C. Image generation
- D. Question answering
